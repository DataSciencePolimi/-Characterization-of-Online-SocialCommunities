{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/giorgiaramponi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import dill\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from gensim.models import CoherenceModel, LsiModel, LdaModel, HdpModel\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import lemmatize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial.distance import cdist\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of the tweets\n",
    "def process_texts(input_texts):\n",
    "\n",
    "    texts = (re.sub((r\"http\\S+\"), \"\", input_texts))\n",
    "    # tokenize\n",
    "    texts =gensim.utils.simple_tokenize(texts)\n",
    "    # lower case\n",
    "    texts = [word.lower() for word in texts]\n",
    "    # remove stopwords\n",
    "    texts = [word for word in texts if word not in stops]   \n",
    "\n",
    "    italianStemmer=SnowballStemmer(\"italian\", ignore_stopwords=True)\n",
    "    texts = [italianStemmer.stem(word) for word in texts] \n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns 2 dictionaries: \n",
    "# - users-texts\n",
    "# - users-domain \n",
    "def load_text(path, domain):\n",
    "    users_dict = {}\n",
    "    domain_dict = {}\n",
    "    dictionary = {}\n",
    "    jsonFile = open(path, 'r')\n",
    "    values = json.load(jsonFile)\n",
    "    jsonFile.close()\n",
    "    for i in values:\n",
    "        for j in values[i]:\n",
    "            # if the user is already in the dictionary we just concatenate the tweets\n",
    "            if (values[i][j]['screen_name'] in users_dict):\n",
    "                prev_text = users_dict.get(values[i][j]['screen_name'])\n",
    "                users_dict[values[i][j]['screen_name']] = prev_text + values[i][j]['text'] + ' '\n",
    "            # otherwise we create a new entry\n",
    "            else:\n",
    "                users_dict[values[i][j]['screen_name']] = values[i][j]['text'] + ' '\n",
    "\n",
    "\n",
    "    for key, value in (users_dict.iteritems()):\n",
    "        tmp = process_texts(value)\n",
    "        pattern = re.compile(\"random*\")\n",
    "        if (pattern.match(domain)):   \n",
    "            if (len(tmp)>=50):\n",
    "                dictionary[key] = tmp\n",
    "                domain_dict[key]=domain\n",
    "        else:\n",
    "            dictionary[key] = tmp\n",
    "            domain_dict[key]=domain\n",
    "\n",
    "\n",
    "    return dictionary, domain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Updates the dictionary of the domain passed (as d)\n",
    "# - Updates the dictionary with all users-texts (domain+random)\n",
    "# - Updates the list of test users\n",
    "# Returns the specific domain dictionary and the list of train users\n",
    "def domain_setup(d):\n",
    "    \n",
    "    domain_specific_dictionary = {}\n",
    "    domain_path = os.path.abspath(\"/Users/giorgiaramponi/Dropbox/gio/ske/tweets/\"+ d +\"_tweets.json\")\n",
    "\n",
    "    #Loads the user-texts dictionary and the users-domain dictionary\n",
    "    domain_specific_dictionary, tmp = load_text(domain_path, d)\n",
    "    \n",
    "    users_domain_dict.update(tmp)\n",
    "    users_texts_dict.update(domain_specific_dictionary)\n",
    "    \n",
    "    # In this case, all users are train users\n",
    "    train_users = domain_specific_dictionary.keys()\n",
    "\n",
    "\n",
    "    return domain_specific_dictionary, train_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds subset dictionary from existing dictionary and list of users\n",
    "def build_dictionary(dictionary, train_u):\n",
    "    new_dict = {}\n",
    "    for u in train_u:\n",
    "        new_dict[u] = dictionary[u]\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_cv(x, c_v):\n",
    "    \n",
    "    plt.plot(x, c_v, label = \"c_v\")\n",
    "    plt.suptitle(\"LDA model evaluation: c_v\")\n",
    "    plt.xlabel(\"Number of topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(\"Coherence_c_v.png\")\n",
    "    plt.show()\n",
    "\n",
    "def graph_uci(x, c_uci):\n",
    "    \n",
    "    plt.plot(x, c_uci, label=\"c_uci\")\n",
    "    plt.suptitle(\"LDA model evaluation: c_uci\")\n",
    "    plt.xlabel(\"Number of topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(\"Coherence_c_uci.png\")\n",
    "    plt.show()\n",
    "    \n",
    "def graph_umass(x, u_mass):\n",
    "    \n",
    "    plt.plot(x, u_mass, label=\"u_mass\")\n",
    "    plt.suptitle(\"LDA model evaluation: u_mass\")\n",
    "    plt.xlabel(\"Number of topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(\"Coherence_u_mass.png\")\n",
    "    plt.show()\n",
    "    \n",
    "def graph_perplexity(x, perplexity):\n",
    "    \n",
    "    plt.plot(x, perplexity, label=\"log_perplexity\")\n",
    "    plt.suptitle(\"LDA model evaluation: perplexity\")\n",
    "    plt.xlabel(\"Number of topics\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(\"Perplexity.png\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates how the coherence changes according to the number of passes performed for the LDA\n",
    "# Input: limit = maximum number of topics\n",
    "def evaluate_num_topics(dictionary, corpus, texts, limit, passes, iterations, random_state):\n",
    "\n",
    "    c_v = []\n",
    "    c_uci = []\n",
    "    u_mass = []\n",
    "    perplexity = []\n",
    "    lm_list = []\n",
    "    for num_top in tqdm(range(1, limit)):\n",
    "        lm = LdaModel(corpus=corpus, num_topics=num_top, id2word=dictionary,  \\\n",
    "                       passes=passes, iterations=iterations, random_state=random_state)\n",
    "        lm_list.append(lm)\n",
    "        cm_cv = CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        cm_umass = CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "        cm_uci = CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_uci')\n",
    "        c_v.append(cm_cv.get_coherence())\n",
    "        c_uci.append(cm_uci.get_coherence())\n",
    "        u_mass.append(cm_umass.get_coherence())\n",
    "        perplexity.append(lm.log_perplexity(corpus))\n",
    "                \n",
    "    # Show graph\n",
    "    x = range(1, limit)\n",
    "    graph_cv(x, c_v)\n",
    "    graph_uci(x, c_uci)\n",
    "    graph_umass(x, u_mass)\n",
    "    graph_perplexity(x, perplexity)\n",
    "    \n",
    "    return lm_list, c_v, c_uci, u_mass, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_topic_scores(num_top, lda_model, user_dict):\n",
    "    \n",
    "    print(\"Number of topics: \"+str(num_top))\n",
    "    for u, t in user_dict.iteritems():\n",
    "        print(\"User: \"+u+ \" from \"+str(users_domain_dict[u]))\n",
    "        bow_vector = train_dictionary.doc2bow(t)\n",
    "        for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "            print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n",
    "        print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_vectors(num_top, lda_model, user_dict):\n",
    "    \n",
    "    feature_vectors = []\n",
    "    for u, t in user_dict.iteritems():\n",
    "        bow_vector = train_dictionary.doc2bow(t)\n",
    "        fv = [0] * num_top\n",
    "        for index, score in lda_model[bow_vector]:\n",
    "            fv[index] = score\n",
    "        feature_vectors.append((u, fv))\n",
    "    \n",
    "    return feature_vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
